> [!tip] ğŸ‰
> å·²æœ‰shè„šæœ¬å¯ä¸€é”®å®‰è£…å®Œæˆ -> [shell](../other/shell.md#hadoopå®Œå…¨åˆ†å¸ƒå¼)


æœ¬æ–‡æ˜¯Hadoopå®Œå…¨åˆ†å¸ƒå¼å®‰è£…æ•™ç¨‹
## ç¯å¢ƒè¯´æ˜

- **ç³»ç»Ÿ**: Ubuntu 24.04  
- **æœåŠ¡å™¨é…ç½®**: 3 å°ä¸»æœº  
- **Hadoop ç‰ˆæœ¬**: 3.4.2  
- **é›†ç¾¤æ¶æ„**: 1 ä¸ª Master èŠ‚ç‚¹ + 2 ä¸ª Slave èŠ‚ç‚¹  
- **èŠ‚ç‚¹é…ç½®**:  
  - hadoop01 (Master): åç§°èŠ‚ç‚¹ + èµ„æºç®¡ç†å™¨  
  - hadoop02 (Slave): æ•°æ®èŠ‚ç‚¹ + èŠ‚ç‚¹ç®¡ç†å™¨  
  - hadoop03 (Slave): æ•°æ®èŠ‚ç‚¹ + èŠ‚ç‚¹ç®¡ç†å™¨  

---

## ç¬¬ä¸€é˜¶æ®µï¼šæ‰€æœ‰èŠ‚ç‚¹åŸºç¡€é…ç½®

### åˆ›å»º hadoop ç”¨æˆ· (åœ¨æ‰€æœ‰ 3 å°æœåŠ¡å™¨æ‰§è¡Œ)

â‘  **åˆ›å»º hadoop ç”¨æˆ·å¹¶æ·»åŠ  sudo æƒé™**

```bash
sudo adduser hadoop
sudo usermod -aG sudo hadoop
````

â‘  **åˆ‡æ¢åˆ° hadoop ç”¨æˆ·**

```bash
su hadoop
```

---

### å®‰è£…åŸºç¡€è½¯ä»¶ (åœ¨æ‰€æœ‰ 3 å°æœåŠ¡å™¨æ‰§è¡Œ)

**æ›´æ–°ç³»ç»Ÿ**

```bash
sudo apt update
sudo apt upgrade -y
```

**å®‰è£… Java ç¯å¢ƒ**

```bash
sudo apt install openjdk-8-jdk -y
```

**é…ç½® JAVA_HOME ç¯å¢ƒå˜é‡**

```bash
echo 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64' >> ~/.bashrc
source ~/.bashrc
```

---

### ä¸‹è½½å¹¶å®‰è£… Hadoop (åœ¨ Master æ‰§è¡Œå³å¯)

â‘  **ä¸‹è½½ Hadoop 3.4.2**

```bash
cd ~
wget https://mirrors.aliyun.com/apache/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz
```

> é˜¿é‡Œäº‘æœåŠ¡å™¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å†…ç½‘ä¼ è¾“
> wget [http://mirrors.cloud.aliyuncs.com/apache/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz](http://mirrors.cloud.aliyuncs.com/apache/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz)

![](https://img.makis-life.cn/images/20251110181548090.png)

â‘  **è§£å‹å¹¶å®‰è£…**

```bash
sudo tar -zxf hadoop-3.4.2.tar.gz -C /usr/local/
sudo mv /usr/local/hadoop-3.4.2 /usr/local/hadoop
sudo chown -R hadoop:hadoop /usr/local/hadoop
```
![](https://img.makis-life.cn/images/20251110181548091.png)

â‘  **é…ç½® Hadoop ç¯å¢ƒå˜é‡**

```bash
vim ~/.bashrc
```

åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ ï¼š

```bash
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
```

â‘  **ä½¿é…ç½®ç”Ÿæ•ˆ**

```bash
source ~/.bashrc
```

---

## ç¬¬äºŒé˜¶æ®µï¼šç½‘ç»œé…ç½®

### è®¾ç½®ä¸»æœºå (åˆ†åˆ«åœ¨å¯¹åº”æœåŠ¡å™¨æ‰§è¡Œ)

**åœ¨ç¬¬ä¸€å°æœåŠ¡å™¨ (Master) æ‰§è¡Œï¼š**

```bash
sudo hostnamectl set-hostname hadoop01
```

**åœ¨ç¬¬äºŒå°æœåŠ¡å™¨ (Slave1) æ‰§è¡Œï¼š**

```bash
sudo hostnamectl set-hostname hadoop02
```

**åœ¨ç¬¬ä¸‰å°æœåŠ¡å™¨ (Slave2) æ‰§è¡Œï¼š**

```bash
sudo hostnamectl set-hostname hadoop03
```

---

### é…ç½®ä¸»æœºåæ˜ å°„

**åœ¨æ‰€æœ‰ 3 å°æœåŠ¡å™¨ä¸Šä¿®æ”¹ hosts æ–‡ä»¶ï¼š**

```bash
sudo vim /etc/hosts
```

æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š

> ä½¿ç”¨ ip addr show è·å– ip åœ°å€

```
ip   hadoop01
ip   hadoop02
ip   hadoop03
```

> ğŸ’¡ **æç¤º**: å¦‚æœäº‘æœåŠ¡å™¨åªæœ‰å…¬ç½‘ IPï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨å…¬ç½‘ IP åœ°å€è¿›è¡Œæ˜ å°„ï¼Œä½†å»ºè®®ä½¿ç”¨å†…ç½‘ IP ä»¥å‡å°‘ç½‘ç»œå»¶è¿Ÿå’Œæµé‡è´¹ç”¨ã€‚
> äº‘æœåŠ¡å™¨éœ€ä½äºåŒä¸€åœ°åŸŸå’Œä¸“ç”¨ç½‘ç»œå†…ã€‚

---

### æµ‹è¯•ç½‘ç»œè¿é€šæ€§

åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šæµ‹è¯•ï¼š

```bash
ping hadoop01 -c 3
ping hadoop02 -c 3
ping hadoop03 -c 3
```


> ç¡®ä¿ä¸‰å°ä¸»æœºäº’ ping æˆåŠŸ

---

## ç¬¬ä¸‰é˜¶æ®µï¼šSSH æ— å¯†ç ç™»å½•é…ç½®

### åœ¨ Master èŠ‚ç‚¹ (hadoop01) æ“ä½œ

â‘  **ç”Ÿæˆ SSH å¯†é’¥**

```bash
cd ~/.ssh || mkdir ~/.ssh && cd ~/.ssh
ssh-keygen -t rsa -P "" -f ~/.ssh/id_rsa
```

â‘  **é…ç½®æœ¬æœºæ— å¯†ç ç™»å½•**

```bash
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys
```

â‘  **å°†å…¬é’¥å¤åˆ¶åˆ° Slave èŠ‚ç‚¹**

```bash
ssh-copy-id hadoop@hadoop02
ssh-copy-id hadoop@hadoop03
```

### æµ‹è¯• SSH æ— å¯†ç ç™»å½•

```bash
ssh hadoop02
ssh hadoop03
```

---

## ç¬¬å››é˜¶æ®µï¼šäº‘æœåŠ¡å™¨å®‰å…¨ç»„é…ç½®

å¦‚æœä½¿ç”¨é˜¿é‡Œäº‘ã€è…¾è®¯äº‘ç­‰äº‘æœåŠ¡å™¨ï¼Œéœ€è¦åœ¨äº‘æ§åˆ¶å°å®‰å…¨ç»„ä¸­å¼€æ”¾ä»¥ä¸‹ç«¯å£ï¼š

| æœåŠ¡               | é»˜è®¤ç«¯å£         |
| ---------------- | ------------ |
| NameNode RPC     | 9000         |
| NameNode WebUI   | 9870         |
| DataNode Data    | 9866         |
| DataNode WebUI   | 9864         |
| Secondary NN     | 9868         |
| ResourceManager  | 8032, 8088   |
| NodeManager      | 8042         |
| JobHistoryServer | 10020, 19888 |

---

## ç¬¬äº”é˜¶æ®µï¼šHadoop é›†ç¾¤é…ç½®

### é…ç½® Hadoop ç¯å¢ƒå˜é‡

```bash
vim /usr/local/hadoop/etc/hadoop/hadoop-env.sh
```

æ·»åŠ æˆ–ä¿®æ”¹ï¼š

```bash
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
```

---

### é…ç½®é›†ç¾¤æ–‡ä»¶ (ä»…åœ¨ hadoop01 æ‰§è¡Œ)

â‘  **workers æ–‡ä»¶**

```bash
vim /usr/local/hadoop/etc/hadoop/workers
```

å†…å®¹ï¼š

```
hadoop02
hadoop03
```

---

### åŒæ­¥é…ç½®åˆ°æ‰€æœ‰èŠ‚ç‚¹

åœ¨ hadoop01 ä¸Šæ‰§è¡Œï¼š

```bash
cd /usr/local
sudo tar -zcf ~/hadoop.master.tar.gz ./hadoop

# ä¼ è¾“åˆ° slave èŠ‚ç‚¹
scp ~/hadoop.master.tar.gz hadoop02:/home/hadoop/
scp ~/hadoop.master.tar.gz hadoop03:/home/hadoop/
```

åœ¨ hadoop02 å’Œ hadoop03 ä¸Šæ‰§è¡Œï¼š

```bash
cd ~
sudo rm -rf /usr/local/hadoop
sudo tar -zxf ~/hadoop.master.tar.gz -C /usr/local/
sudo chown -R hadoop:hadoop /usr/local/hadoop
```

## é…ç½®æ–‡ä»¶

- **core-site.xml**
    

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop01:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/usr/local/hadoop/tmp</value>
    </property>
</configuration>

```

- **yarn-site.xml**
    

```xml
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hadoop01</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>hadoop01:8088</value>
    </property>
</configuration>
```

- **hdfs-site.xml**
    

```xml
<configuration>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>hadoop01:50090</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/usr/local/hadoop/tmp/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/usr/local/hadoop/tmp/dfs/data</value>
    </property>
    <property>
        <name>dfs.namenode.http-address</name>
        <value>hadoop01:9870</value>
    </property>
</configuration>
```

- **mapred-site.xml**
    

```xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>hadoop01:10020</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>hadoop01:19888</value>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
    </property>
    <property>
        <name>mapreduce.map.env</name>
        <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
    </property>
    <property>
        <name>mapreduce.reduce.env</name>
        <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
    </property>
</configuration>
```

## ç¬¬å…­é˜¶æ®µï¼šå¯åŠ¨é›†ç¾¤

### æ ¼å¼åŒ– NameNode (ä»…ç¬¬ä¸€æ¬¡ï¼Œåœ¨ hadoop01 æ‰§è¡Œ)

```bash
hdfs namenode -format
```

### å¯åŠ¨é›†ç¾¤æœåŠ¡ (åœ¨ hadoop01 æ‰§è¡Œ)

```bash
start-dfs.sh
start-yarn.sh
mapred --daemon start historyserver
```

### éªŒè¯é›†ç¾¤çŠ¶æ€

â‘ **æ£€æŸ¥è¿›ç¨‹**

```bash
# åœ¨hadoop01ä¸Šæ‰§è¡Œ
jps

## NameNode, SecondaryNameNode, ResourceManager, JobHistoryServer

# åœ¨hadoop02å’Œhadoop03ä¸Šæ‰§è¡Œ
jps
## DataNode, NodeManager

# ä»¥ä¸Šå°‘ä¸€ä¸ªéƒ½æ˜¯æŠ¥é”™
```

â‘ **æ£€æŸ¥ HDFS çŠ¶æ€**

```bash
hdfs dfsadmin -report
```

è¾“å‡ºï¼ˆåªè¦LiveNodeä¸ç­‰äº0å°±æ˜¯æˆåŠŸäº†ï¼‰

```
Configured Capacity: 83765886976 (78.01 GB)
Present Capacity: 61911212032 (57.66 GB)
DFS Remaining: 61911162880 (57.66 GB)
DFS Used: 49152 (48 KB)
DFS Used%: 0.00%
Replicated Blocks:
        Under replicated blocks: 0
        Blocks with corrupt replicas: 0
        Missing blocks: 0
        Missing blocks (with replication factor 1): 0
        Low redundancy blocks with highest priority to recover: 0
        Pending deletion blocks: 0
Erasure Coded Block Groups:
        Low redundancy block groups: 0
        Block groups with corrupt internal blocks: 0
        Missing block groups: 0
        Low redundancy blocks with highest priority to recover: 0
        Pending deletion blocks: 0

-------------------------------------------------
Live datanodes (2):

Name: 120.2xx.1x.1xx:9866 (hadoop02)
Hostname: hadoop02
Decommission Status : Normal
Configured Capacity: 41882943488 (39.01 GB)
DFS Used: 24576 (24 KB)
Non DFS Used: 11279278080 (10.50 GB)
DFS Remaining: 28667904000 (26.70 GB)
DFS Used%: 0.00%
DFS Remaining%: 68.45%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 0
Last contact: Sun Sep 21 03:07:57 CST 2025
Last Block Report: Sun Sep 21 03:00:42 CST 2025
Num of Blocks: 0


Name: 47.11x.xx8.xxx:9866 (hadoop03)
Hostname: hadoop03
Decommission Status : Normal
Configured Capacity: 41882943488 (39.01 GB)
DFS Used: 24576 (24 KB)
Non DFS Used: 6703923200 (6.24 GB)
DFS Remaining: 33243258880 (30.96 GB)
DFS Used%: 0.00%
DFS Remaining%: 79.37%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 0
Last contact: Sun Sep 21 03:07:55 CST 2025
Last Block Report: Sun Sep 21 03:00:37 CST 2025
Num of Blocks: 0
```

â‘ **Web ç•Œé¢è®¿é—®**

- NameNode: `http://IP:9870`
- ResourceManager: `http://IP:8088`
- JobHistory: `http://IP:19888`

## ç¬¬ä¸ƒé˜¶æ®µï¼šæµ‹è¯• MapReduce

### åˆ›å»ºæµ‹è¯•ç›®å½•å’Œæ–‡ä»¶

```bash
hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/hadoop
hdfs dfs -mkdir input
hdfs dfs -put $HADOOP_HOME/etc/hadoop/*.xml input
```

### è¿è¡Œè¯é¢‘ç»Ÿè®¡ç¤ºä¾‹

```bash
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.2.jar wordcount input output
```
![](https://img.makis-life.cn/images/20251110181548092.png)

### æŸ¥çœ‹ç»“æœ

scat output/part-r-00000

## å…³é—­é›†ç¾¤

åœ¨ hadoop01 æ‰§è¡Œï¼š

```bash
mapred --daemon stop historyserver
stop-yarn.sh
stop-dfs.sh
```
